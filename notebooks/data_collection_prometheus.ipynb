{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Scheduler Data Collection - HYDATIS Cluster\n",
    "## Collecte Données Historiques Prometheus\n",
    "\n",
    "**Environnement**: Jupyter Notebook HYDATIS - wassimmezrani namespace  \n",
    "**Cluster**: 6 nodes (3 masters + 3 workers)  \n",
    "**Prometheus**: http://10.110.190.83:9090  \n",
    "\n",
    "**Objectif**: Collecter et préparer données pour 3 algorithmes ML:\n",
    "- XGBoost: Patterns temporels charge des nodes\n",
    "- Q-Learning: Historique placements + performances\n",
    "- Isolation Forest: Comportements normaux/anormaux nodes\n",
    "\n",
    "**Volumes montés**:\n",
    "- `/data` (50Gi): Données historiques\n",
    "- `/models` (20Gi): Modèles ML\n",
    "- `/experiments` (15Gi): Expérimentations\n",
    "- `/home/jovyan` (19Gi): Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration et imports pour environnement Jupyter HYDATIS\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nimport json\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom concurrent.futures import ThreadPoolExecutor\nimport os\nfrom pathlib import Path\n\nwarnings.filterwarnings('ignore')\nplt.style.use('default')\n\n# Configuration Prometheus HYDATIS Cluster\nPROMETHEUS_URL = \"http://prometheus-k8s.monitoring.svc.cluster.local:9090\"\nPROMETHEUS_EXTERNAL_URL = \"http://10.110.190.83:9090\"\n\n# Configuration timeout et performance pour environnement production\nREQUEST_TIMEOUT = 120\nCOLLECTION_PAUSE = 2.0\nMAX_RETRIES = 3\n\n# Configuration chemins pour environnement Jupyter\nDATA_PATH = Path(\"/data\")\nMODELS_PATH = Path(\"/models\")\nEXPERIMENTS_PATH = Path(\"/experiments\")\nWORKSPACE_PATH = Path(\"/home/jovyan\")\n\n# Vérifier les volumes montés\nprint(\"=\" * 60)\nprint(\"ML-SCHEDULER DATA COLLECTION - HYDATIS CLUSTER\")\nprint(\"=\" * 60)\nprint(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Environnement: Jupyter Notebook (wassimmezrani)\")\nprint(f\"\\nVolumes vérifiés:\")\nfor path_name, path in [(\"DATA\", DATA_PATH), (\"MODELS\", MODELS_PATH), \n                        (\"EXPERIMENTS\", EXPERIMENTS_PATH), (\"WORKSPACE\", WORKSPACE_PATH)]:\n    exists = path.exists()\n    if exists:\n        # Vérifier espace disponible\n        try:\n            stat_info = os.statvfs(path)\n            available_gb = (stat_info.f_bavail * stat_info.f_frsize) / (1024**3)\n            print(f\"  {path_name:12} {path}: OK ({available_gb:.1f}GB disponible)\")\n        except:\n            print(f\"  {path_name:12} {path}: OK\")\n    else:\n        print(f\"  {path_name:12} {path}: ERREUR\")\n\n# Créer répertoires si nécessaire\nml_data_dir = DATA_PATH / \"ml_scheduler_data\"\nml_data_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"\\nRépertoire données ML: {ml_data_dir}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion et Validation Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_prometheus_connection():\n    \"\"\"Test connexion Prometheus avec URLs internes et externes optimisé pour HYDATIS\"\"\"\n    # Priorité LoadBalancer externe car plus fiable en environnement Jupyter\n    urls_to_test = [\n        (PROMETHEUS_EXTERNAL_URL, \"LoadBalancer externe\"),\n        (PROMETHEUS_URL, \"Service interne cluster\")\n    ]\n    \n    for url, description in urls_to_test:\n        for attempt in range(MAX_RETRIES):\n            try:\n                print(f\"Test {description}: {url} (tentative {attempt + 1}/{MAX_RETRIES})\")\n                response = requests.get(f\"{url}/api/v1/query?query=up\", timeout=REQUEST_TIMEOUT//4)\n                \n                if response.status_code == 200:\n                    data = response.json()\n                    if data['status'] == 'success':\n                        services_up = len(data['data']['result'])\n                        print(f\"  SUCCESS: Connexion réussie\")\n                        print(f\"  Services UP: {services_up}\")\n                        \n                        # Vérification additionnelle de la disponibilité des métriques\n                        test_query = \"node_cpu_seconds_total\"\n                        test_response = requests.get(f\"{url}/api/v1/query?query={test_query}\", \n                                                   timeout=REQUEST_TIMEOUT//4)\n                        if test_response.status_code == 200 and test_response.json()['status'] == 'success':\n                            print(f\"  OK: Métriques nodes disponibles\")\n                            return url\n                        else:\n                            print(f\"  WARNING: Connexion OK mais métriques limitées\")\n                            return url\n                else:\n                    print(f\"  ERROR: HTTP {response.status_code}\")\n                    \n            except requests.exceptions.RequestException as e:\n                print(f\"  ERROR tentative {attempt + 1}: {e}\")\n                if attempt < MAX_RETRIES - 1:\n                    time.sleep(2)\n                continue\n    \n    print(\"ERROR: Aucune connexion Prometheus disponible après tous les essais\")\n    return None\n\n# Test connexion avec retry\nactive_prometheus_url = test_prometheus_connection()\nif active_prometheus_url:\n    print(f\"\\nSUCCESS: Prometheus actif: {active_prometheus_url}\")\n    print(f\"Configuration optimisée pour cluster HYDATIS\")\nelse:\n    print(f\"\\nERROR: Prometheus inaccessible\")\n    print(f\"Vérifier la connectivité réseau et les services LoadBalancer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_cluster_metrics():\n    \"\"\"Vérifier métriques disponibles du cluster HYDATIS\"\"\"\n    if not active_prometheus_url:\n        print(\"ERROR: Pas de connexion Prometheus\")\n        return False\n        \n    print(\"Vérification métriques cluster HYDATIS...\")\n    \n    # Tester métriques essentielles\n    essential_queries = {\n        \"Nodes cluster\": \"kube_node_info\",\n        \"CPU nodes\": \"node_cpu_seconds_total\",\n        \"Mémoire nodes\": \"node_memory_MemAvailable_bytes\",\n        \"Pods running\": \"kube_pod_status_phase{phase='Running'}\",\n        \"Load average\": \"node_load1\"\n    }\n    \n    results = {}\n    for name, query in essential_queries.items():\n        try:\n            response = requests.get(f\"{active_prometheus_url}/api/v1/query\", \n                                  params={'query': query}, timeout=10)\n            if response.status_code == 200:\n                data = response.json()\n                if data['status'] == 'success' and data['data']['result']:\n                    count = len(data['data']['result'])\n                    results[name] = count\n                    print(f\"  OK {name}: {count} métriques\")\n                else:\n                    results[name] = 0\n                    print(f\"  ERROR {name}: Aucune donnée\")\n            else:\n                results[name] = 0\n                print(f\"  ERROR {name}: HTTP {response.status_code}\")\n        except Exception as e:\n            results[name] = 0\n            print(f\"  ERROR {name}: {e}\")\n    \n    # Vérifier disponibilité données historiques\n    end_time = datetime.now()\n    start_time = end_time - timedelta(days=15)  # Tester 15 jours\n    \n    try:\n        params = {\n            'query': 'node_cpu_seconds_total',\n            'start': start_time.timestamp(),\n            'end': end_time.timestamp(),\n            'step': '1h'\n        }\n        response = requests.get(f\"{active_prometheus_url}/api/v1/query_range\", \n                              params=params, timeout=30)\n        \n        if response.status_code == 200:\n            data = response.json()\n            if data['status'] == 'success' and data['data']['result']:\n                values = data['data']['result'][0]['values']\n                if values:\n                    first_ts = datetime.fromtimestamp(float(values[0][0]))\n                    last_ts = datetime.fromtimestamp(float(values[-1][0]))\n                    days_span = (last_ts - first_ts).days\n                    print(f\"  OK Données historiques: {days_span} jours disponibles\")\n                    print(f\"    Période: {first_ts.strftime('%Y-%m-%d')} → {last_ts.strftime('%Y-%m-%d')}\")\n                    results['historical_days'] = days_span\n                else:\n                    print(f\"  ERROR Données historiques: Aucune valeur\")\n                    results['historical_days'] = 0\n            else:\n                print(f\"  ERROR Données historiques: Pas de résultats\")\n                results['historical_days'] = 0\n    except Exception as e:\n        print(f\"  ERROR Données historiques: {e}\")\n        results['historical_days'] = 0\n    \n    # Évaluer la qualité\n    total_metrics = sum(1 for v in results.values() if isinstance(v, int) and v > 0)\n    cluster_ready = total_metrics >= 4 and results.get('historical_days', 0) >= 7\n    \n    print(f\"\\nÉvaluation cluster:\")\n    print(f\"  Métriques disponibles: {total_metrics}/5\")\n    print(f\"  Données historiques: {results.get('historical_days', 0)} jours\")\n    print(f\"  Status: {'READY' if cluster_ready else 'NOT READY'}\")\n    \n    return cluster_ready, results\n\n# Vérification cluster\ncluster_ready, cluster_metrics = check_cluster_metrics()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration des Métriques ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration métriques optimisée pour cluster HYDATIS\n",
    "ML_METRICS_CONFIG = {\n",
    "    'xgboost_features': {\n",
    "        'description': 'Prédiction charge temporelle nodes HYDATIS',\n",
    "        'metrics': [\n",
    "            'node_cpu_seconds_total',\n",
    "            'node_memory_MemAvailable_bytes',\n",
    "            'node_memory_MemTotal_bytes',\n",
    "            'node_filesystem_avail_bytes{mountpoint=\"/\"}',\n",
    "            'node_load1',\n",
    "            'node_load5',\n",
    "            'node_load15',\n",
    "            'node_network_receive_bytes_total',\n",
    "            'node_network_transmit_bytes_total'\n",
    "        ],\n",
    "        'storage_path': DATA_PATH / \"xgboost_data\"\n",
    "    },\n",
    "    'qlearning_features': {\n",
    "        'description': 'Optimisation scheduling pods cluster HYDATIS',\n",
    "        'metrics': [\n",
    "            'kube_pod_info',\n",
    "            'kube_pod_status_phase',\n",
    "            'container_cpu_usage_seconds_total',\n",
    "            'container_memory_usage_bytes',\n",
    "            'kube_pod_container_resource_requests',\n",
    "            'kube_pod_container_resource_limits'\n",
    "        ],\n",
    "        'storage_path': DATA_PATH / \"qlearning_data\"\n",
    "    },\n",
    "    'isolation_features': {\n",
    "        'description': 'Détection anomalies comportement nodes HYDATIS',\n",
    "        'metrics': [\n",
    "            'node_cpu_seconds_total',\n",
    "            'node_memory_MemAvailable_bytes',\n",
    "            'node_network_receive_bytes_total',\n",
    "            'node_network_transmit_bytes_total',\n",
    "            'node_filesystem_avail_bytes{mountpoint=\"/\"}',\n",
    "            'node_load1',\n",
    "            'node_load5'\n",
    "        ],\n",
    "        'storage_path': DATA_PATH / \"isolation_data\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Créer répertoires pour chaque algorithme\n",
    "print(\"Configuration stockage données ML:\")\n",
    "for algo_name, config in ML_METRICS_CONFIG.items():\n",
    "    storage_path = config['storage_path']\n",
    "    storage_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  {algo_name:20} → {storage_path}\")\n",
    "    print(f\"  {'':20}   {config['description']}\")\n",
    "    print(f\"  {'':20}   {len(config['metrics'])} métriques configurées\")\n",
    "\n",
    "print(f\"\\nTotal algorithmes: {len(ML_METRICS_CONFIG)}\")\n",
    "print(f\"Total métriques uniques: {len(set().union(*[config['metrics'] for config in ML_METRICS_CONFIG.values()]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte Données Nodes (XGBoost + Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "                        # Identifier si master ou worker\n                        if node_name.startswith('10.110.190.'):\n                            node_ip = node_name\n                            if node_ip in ['10.110.190.32', '10.110.190.33', '10.110.190.34']:\n                                node_role = 'master'\n                                ip_mapping = {'32': '1', '33': '2', '34': '3'}\n                                node_name = f\"master{ip_mapping[node_ip.split('.')[-1]]}\"\n                            elif node_ip in ['10.110.190.35', '10.110.190.36', '10.110.190.37']:\n                                node_role = 'worker'\n                                ip_mapping = {'35': '1', '36': '2', '37': '3'}\n                                node_name = f\"worker{ip_mapping[node_ip.split('.')[-1]]}\"\n                            else:\n                                node_role = 'unknown'\n                        else:\n                            node_role = 'unknown'\n                            node_ip = 'unknown'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte Données Pods (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def collect_pod_scheduling_data_optimized(days_back=15, step_hours=2):\n    \"\"\"Collecte données pods optimisée pour Q-Learning HYDATIS\"\"\"\n    if not active_prometheus_url:\n        return pd.DataFrame()\n    \n    end_time = datetime.now()\n    start_time = end_time - timedelta(days=days_back)\n    \n    print(f\"\\nCOLLECTE DONNÉES PODS SCHEDULING\")\n    print(f\"Période: {start_time.strftime('%Y-%m-%d %H:%M')} → {end_time.strftime('%Y-%m-%d %H:%M')}\")\n    print(f\"Résolution: {step_hours}h pour réduire le volume\")\n    \n    # Métriques pods essentielles pour Q-Learning\n    pod_metrics = [\n        ('kube_pod_info', 'Pod Information'),\n        ('kube_pod_status_phase{phase=\"Running\"}', 'Running Pods'),\n        ('container_cpu_usage_seconds_total', 'Container CPU'),\n        ('container_memory_usage_bytes', 'Container Memory'),\n        ('kube_pod_container_resource_requests{resource=\"cpu\"}', 'CPU Requests'),\n        ('kube_pod_container_resource_requests{resource=\"memory\"}', 'Memory Requests')\n    ]\n    \n    all_pod_data = []\n    \n    for i, (metric_name, description) in enumerate(pod_metrics, 1):\n        print(f\"[{i}/{len(pod_metrics)}] Collecte {description}...\")\n        \n        params = {\n            'query': metric_name,\n            'start': start_time.timestamp(),\n            'end': end_time.timestamp(),\n            'step': f'{step_hours}h'\n        }\n        \n        try:\n            response = requests.get(f\"{active_prometheus_url}/api/v1/query_range\", \n                                  params=params, timeout=180)\n            \n            if response.status_code == 200:\n                data = response.json()\n                if data['status'] == 'success' and data['data']['result']:\n                    series_count = 0\n                    points_count = 0\n                    \n                    for series in data['data']['result']:\n                        labels = series['metric']\n                        pod_name = labels.get('pod', 'unknown')\n                        namespace = labels.get('namespace', 'unknown')\n                        node_name = labels.get('node', labels.get('instance', 'unknown'))\n                        container = labels.get('container', '')\n                        resource = labels.get('resource', '')\n                        \n                        # Nettoyer nom node\n                        if ':' in node_name:\n                            node_name = node_name.split(':')[0]\n                        \n                        # Mapper IP vers nom node\n                        if node_name.startswith('10.110.190.'):\n                            node_mapping = {\n                                '10.110.190.32': 'master1',\n                                '10.110.190.33': 'master2', \n                                '10.110.190.34': 'master3',\n                                '10.110.190.35': 'worker1',\n                                '10.110.190.36': 'worker2',\n                                '10.110.190.37': 'worker3'\n                            }\n                            node_name = node_mapping.get(node_name, node_name)\n                        \n                        series_count += 1\n                        \n                        for timestamp_str, value_str in series['values']:\n                            try:\n                                timestamp = datetime.fromtimestamp(float(timestamp_str))\n                                value = float(value_str)\n                                points_count += 1\n                                \n                                all_pod_data.append({\n                                    'timestamp': timestamp,\n                                    'pod_name': pod_name,\n                                    'namespace': namespace,\n                                    'node_name': node_name,\n                                    'container': container,\n                                    'resource': resource,\n                                    'metric': metric_name.split('{')[0],\n                                    'value': value,\n                                    'description': description\n                                })\n                            except (ValueError, TypeError):\n                                # Pour les métriques de statut\n                                if value_str in ['1', '0']:\n                                    value = int(value_str)\n                                    points_count += 1\n                                    all_pod_data.append({\n                                        'timestamp': timestamp,\n                                        'pod_name': pod_name,\n                                        'namespace': namespace,\n                                        'node_name': node_name,\n                                        'container': container,\n                                        'resource': resource,\n                                        'metric': metric_name.split('{')[0],\n                                        'value': value,\n                                        'description': description\n                                    })\n                                continue\n                    \n                    print(f\"    OK {series_count} séries, {points_count} points\")\n                else:\n                    print(f\"    ERROR Pas de données\")\n            else:\n                print(f\"    ERROR HTTP {response.status_code}\")\n                \n        except Exception as e:\n            print(f\"    ERROR Erreur: {e}\")\n        \n        # Pause optimisée pour collecte pods (plus de données)\n        time.sleep(COLLECTION_PAUSE * 1.5)\n    \n    if all_pod_data:\n        df = pd.DataFrame(all_pod_data)\n        \n        print(f\"\\nRÉSULTATS COLLECTE PODS:\")\n        print(f\"  Total points: {len(df):,}\")\n        print(f\"  Pods uniques: {df['pod_name'].nunique()}\")\n        print(f\"  Namespaces: {df['namespace'].nunique()}\")\n        print(f\"  Nodes: {df[df['node_name'] != 'unknown']['node_name'].nunique()}\")\n        print(f\"  Métriques: {df['metric'].nunique()}\")\n        \n        # Top namespaces\n        top_ns = df['namespace'].value_counts().head(5)\n        print(f\"\\n  Top namespaces:\")\n        for ns, count in top_ns.items():\n            print(f\"    {ns:20}: {count:,} points\")\n        \n        return df\n    else:\n        print(\"\\nERROR Aucune donnée pod collectée\")\n        return pd.DataFrame()\n\n# Lancement collecte pods\nif cluster_ready:\n    pod_scheduling_df = collect_pod_scheduling_data_optimized(days_back=15)\nelse:\n    pod_scheduling_df = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Qualité et Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_and_save_data():\n    \"\"\"Analyse qualité et sauvegarde optimisée pour environnement HYDATIS\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ANALYSE QUALITÉ DONNÉES ML-SCHEDULER HYDATIS\")\n    print(\"=\"*60)\n    \n    datasets = {\n        'Node Metrics': node_metrics_df,\n        'Pod Scheduling': pod_scheduling_df\n    }\n    \n    quality_report = {}\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    for name, df in datasets.items():\n        print(f\"\\n{name.upper()}:\")\n        \n        if df.empty:\n            print(\"   ERROR Dataset vide\")\n            quality_report[name] = {'status': 'EMPTY', 'score': 0}\n            continue\n        \n        # Métriques de base\n        total_rows = len(df)\n        unique_timestamps = df['timestamp'].nunique()\n        date_range = (df['timestamp'].max() - df['timestamp'].min()).days\n        \n        print(f\"   Lignes: {total_rows:,}\")\n        print(f\"   Timestamps uniques: {unique_timestamps:,}\")\n        print(f\"   Période couverte: {date_range} jours\")\n        \n        # Analyse spécifique par type\n        if 'node_name' in df.columns:\n            print(f\"   Nodes HYDATIS: {df['node_name'].nunique()}\")\n            print(f\"   Métriques: {df['metric'].nunique()}\")\n        elif 'pod_name' in df.columns:\n            print(f\"   Pods: {df['pod_name'].nunique()}\")\n            print(f\"   Namespaces: {df['namespace'].nunique()}\")\n        \n        # Complétude\n        missing_values = df.isnull().sum().sum()\n        completeness = ((total_rows * len(df.columns) - missing_values) / (total_rows * len(df.columns))) * 100\n        print(f\"   Complétude: {completeness:.1f}%\")\n        \n        # Score qualité adapté HYDATIS\n        score = 0\n        if total_rows > 1000: score += 30  # Volume suffisant\n        if date_range >= 7: score += 25    # Période minimum MLOps\n        if date_range >= 14: score += 10   # Période optimale\n        if completeness >= 95: score += 25 # Complétude excellente\n        elif completeness >= 90: score += 20\n        if unique_timestamps > 100: score += 10  # Diversité temporelle\n        \n        status = \"EXCELLENT\" if score >= 85 else \"BON\" if score >= 70 else \"MOYEN\" if score >= 50 else \"FAIBLE\"\n        print(f\"   Score qualité: {score}/100 - {status}\")\n        \n        quality_report[name] = {\n            'status': status,\n            'score': score,\n            'rows': total_rows,\n            'completeness': completeness,\n            'days': date_range\n        }\n        \n        # Sauvegarde dans volumes appropriés\n        if score >= 50:  # Seulement si qualité suffisante\n            dataset_name = name.lower().replace(' ', '_')\n            \n            # Sauvegarde CSV dans /data\n            csv_path = DATA_PATH / f\"{dataset_name}_{timestamp}.csv\"\n            df.to_csv(csv_path, index=False)\n            print(f\"   Sauvegardé CSV: {csv_path}\")\n            \n            # Sauvegarde Parquet pour performance\n            parquet_path = DATA_PATH / f\"{dataset_name}_{timestamp}.parquet\"\n            df.to_parquet(parquet_path, index=False)\n            print(f\"   Sauvegardé Parquet: {parquet_path}\")\n            \n            # Échantillon pour experiments\n            if len(df) > 10000:\n                sample_df = df.sample(n=min(5000, len(df)//10))\n                sample_path = EXPERIMENTS_PATH / f\"{dataset_name}_sample_{timestamp}.csv\"\n                sample_df.to_csv(sample_path, index=False)\n                print(f\"   Échantillon: {sample_path} ({len(sample_df)} lignes)\")\n    \n    # Score global et métadonnées\n    if quality_report:\n        valid_scores = [r['score'] for r in quality_report.values() if r['score'] > 0]\n        if valid_scores:\n            avg_score = sum(valid_scores) / len(valid_scores)\n            global_status = \"READY\" if avg_score >= 60 else \"NEEDS_IMPROVEMENT\"\n            \n            print(f\"\\nSCORE GLOBAL HYDATIS: {avg_score:.1f}/100 - {global_status}\")\n            print(f\"Prêt pour développement ML: {'SUCCESS OUI' if avg_score >= 60 else 'ERROR NON'}\")\n            \n            # Sauvegarde métadonnées complètes\n            metadata = {\n                'collection_info': {\n                    'date': datetime.now().isoformat(),\n                    'cluster': 'HYDATIS',\n                    'environment': 'Jupyter Notebook wassimmezrani',\n                    'prometheus_url': active_prometheus_url\n                },\n                'cluster_info': cluster_metrics,\n                'data_quality': quality_report,\n                'ml_algorithms_config': ML_METRICS_CONFIG,\n                'datasets_summary': {\n                    name: {\n                        'rows': len(df) if not df.empty else 0,\n                        'columns': list(df.columns) if not df.empty else [],\n                        'date_range': {\n                            'start': df['timestamp'].min().isoformat() if not df.empty and 'timestamp' in df.columns else None,\n                            'end': df['timestamp'].max().isoformat() if not df.empty and 'timestamp' in df.columns else None\n                        } if not df.empty else None\n                    } for name, df in datasets.items()\n                },\n                'storage_paths': {\n                    'data': str(DATA_PATH),\n                    'models': str(MODELS_PATH),\n                    'experiments': str(EXPERIMENTS_PATH)\n                }\n            }\n            \n            metadata_path = DATA_PATH / f\"ml_scheduler_metadata_{timestamp}.json\"\n            with open(metadata_path, 'w') as f:\n                json.dump(metadata, f, indent=2, default=str)\n            \n            print(f\"\\nMétadonnées complètes: {metadata_path}\")\n            \n            return avg_score >= 60, metadata\n    \n    return False, {}\n\n# Analyse finale et sauvegarde\nml_ready, collection_metadata = analyze_and_save_data()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé et Prochaines Étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Résumé final de la collecte\nprint(\"\\n\" + \"=\"*60)\nprint(\"RÉSUMÉ COLLECTE DONNÉES ML-SCHEDULER HYDATIS\")\nprint(\"=\"*60)\n\nprint(f\"\\nENVIRONNEMENT:\")\nprint(f\"  Cluster: HYDATIS (6 nodes: 3 masters + 3 workers)\")\nprint(f\"  Jupyter: wassimmezrani namespace\")\nprint(f\"  Prometheus: {active_prometheus_url if active_prometheus_url else 'Non disponible'}\")\nprint(f\"  Date collecte: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\nprint(f\"\\nDONNÉES COLLECTÉES:\")\nif not node_metrics_df.empty:\n    print(f\"  SUCCESS Node Metrics: {len(node_metrics_df):,} points\")\n    print(f\"     - Nodes HYDATIS: {node_metrics_df['node_name'].nunique()}\")\n    print(f\"     - Métriques: {node_metrics_df['metric'].nunique()}\")\n    print(f\"     - Période: {(node_metrics_df['timestamp'].max() - node_metrics_df['timestamp'].min()).days} jours\")\nelse:\n    print(f\"  ERROR Node Metrics: Aucune donnée collectée\")\n\nif not pod_scheduling_df.empty:\n    print(f\"  SUCCESS Pod Scheduling: {len(pod_scheduling_df):,} points\")\n    print(f\"     - Pods uniques: {pod_scheduling_df['pod_name'].nunique()}\")\n    print(f\"     - Namespaces: {pod_scheduling_df['namespace'].nunique()}\")\n    print(f\"     - Métriques: {pod_scheduling_df['metric'].nunique()}\")\nelse:\n    print(f\"  ERROR Pod Scheduling: Aucune donnée collectée\")\n\nprint(f\"\\nSTOCKAGE:\")\nif ml_ready:\n    print(f\"  SUCCESS Données sauvegardées dans {DATA_PATH}\")\n    print(f\"  SUCCESS Échantillons dans {EXPERIMENTS_PATH}\")\n    print(f\"  SUCCESS Métadonnées complètes générées\")\nelse:\n    print(f\"  ERROR Qualité insuffisante - sauvegarde partielle\")\n\nprint(f\"\\nSTATUS FINAL:\")\nif ml_ready:\n    print(f\"  SUCCESS READY FOR ML DEVELOPMENT\")\n    print(f\"  \\nPROCHAINES ÉTAPES:\")\n    print(f\"    1. Développer XGBoost Predictor (prédiction charge nodes)\")\n    print(f\"    2. Développer Q-Learning Optimizer (optimisation scheduling)\")\n    print(f\"    3. Développer Isolation Forest Detector (détection anomalies)\")\n    print(f\"    4. Intégrer algorithmes dans ML-Scheduler\")\n    print(f\"    5. Tests et validation sur cluster HYDATIS\")\n    print(f\"    6. Déploiement production\")\nelse:\n    print(f\"  WARNING NEEDS IMPROVEMENT\")\n    print(f\"  \\nACTIONS REQUISES:\")\n    print(f\"    1. Vérifier connectivité Prometheus\")\n    print(f\"    2. Augmenter période de rétention données\")\n    print(f\"    3. Relancer collecte avec paramètres ajustés\")\n\nprint(f\"\\nFICHIERS DISPONIBLES:\")\ndata_files = list(DATA_PATH.glob(\"*\"))\nif data_files:\n    print(f\"  Répertoire /data: {len(data_files)} fichiers\")\n    for file_path in sorted(data_files)[-5:]:  # Afficher les 5 derniers\n        size_mb = file_path.stat().st_size / (1024*1024) if file_path.is_file() else 0\n        print(f\"    {file_path.name} ({size_mb:.1f} MB)\")\n\nexperiment_files = list(EXPERIMENTS_PATH.glob(\"*\"))\nif experiment_files:\n    print(f\"  Répertoire /experiments: {len(experiment_files)} fichiers\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"COLLECTE {'TERMINÉE AVEC SUCCÈS' if ml_ready else 'PARTIELLEMENT RÉUSSIE'}\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}