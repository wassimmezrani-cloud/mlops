#!/usr/bin/env python3
"""
Ex√©cution Pipeline ML-Scheduler - √âtape 7
D√©monstration compl√®te du pipeline Kubeflow orchestr√©
Simulation ex√©cution end-to-end des 5 composants
"""

import os
import sys
import json
import time
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging

# Add notebooks to path for imports
sys.path.append('./notebooks')

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PipelineExecutor:
    """
    Ex√©cution compl√®te du pipeline ML-Scheduler
    Simulation des 5 composants en s√©quence
    """
    
    def __init__(self):
        """Initialize pipeline executor"""
        self.pipeline_config = {
            'pipeline_name': 'ml-scheduler-training-pipeline',
            'version': '1.0.0',
            'execution_id': f"exec_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'start_time': datetime.now(),
            'components': [
                'data_collection',
                'preprocessing', 
                'trio_training',
                'trio_validation',
                'kserve_deployment'
            ]
        }
        
        self.execution_results = {
            'pipeline_id': self.pipeline_config['execution_id'],
            'status': 'RUNNING',
            'components_results': {},
            'performance_metrics': {},
            'errors': []
        }
        
        # Create execution directory
        self.execution_dir = f"./pipeline_executions/{self.pipeline_config['execution_id']}"
        os.makedirs(self.execution_dir, exist_ok=True)
        
        logger.info(f"Pipeline Executor initialized - ID: {self.pipeline_config['execution_id']}")
    
    def simulate_data_collection(self) -> Dict[str, Any]:
        """
        Simulate Component 1: Data Collection from Prometheus
        """
        logger.info("üîÑ STEP 1: Data Collection Component - Starting...")
        step_start = time.time()
        
        try:
            # Simulate Prometheus data collection
            collection_config = {
                'prometheus_endpoint': 'http://prometheus.monitoring.svc.cluster.local:9090',
                'collection_period_days': 30,
                'metrics_collected': [
                    'node_cpu_seconds_total',
                    'node_memory_MemAvailable_bytes', 
                    'node_load1',
                    'kube_pod_info',
                    'container_cpu_usage_seconds_total'
                ]
            }
            
            # Generate synthetic historical data
            logger.info("  üìä Collecting 30 days historical metrics...")
            time.sleep(2)  # Simulate collection time
            
            # Create realistic dataset
            end_time = datetime.now()
            start_time = end_time - timedelta(days=30)
            time_points = pd.date_range(start_time, end_time, freq='5min')
            
            synthetic_data = []
            for timestamp in time_points[:100]:  # Limit for demo
                for node_id in [f'worker-{i}' for i in range(1, 7)]:
                    # Business hours pattern
                    hour = timestamp.hour
                    business_factor = 1.0 if 8 <= hour <= 18 else 0.6
                    
                    data_point = {
                        'timestamp': timestamp.isoformat(),
                        'node_id': node_id,
                        'cpu_utilization': max(0, min(1, np.random.normal(0.4 * business_factor, 0.15))),
                        'memory_utilization': max(0, min(1, np.random.normal(0.5 * business_factor, 0.1))),
                        'load_1min': max(0, np.random.normal(1.5 * business_factor, 0.5)),
                        'pod_count': max(0, int(np.random.poisson(15 * business_factor))),
                        'disk_usage_percent': max(0, min(100, np.random.normal(45, 10)))
                    }
                    synthetic_data.append(data_point)
            
            # Save dataset
            df = pd.DataFrame(synthetic_data)
            dataset_path = f"{self.execution_dir}/raw_dataset.parquet"
            df.to_parquet(dataset_path, index=False)
            
            # Quality assessment
            quality_metrics = {
                'total_records': len(df),
                'unique_nodes': df['node_id'].nunique(),
                'time_span_hours': 30 * 24,
                'data_completeness': 1.0 - df.isnull().sum().sum() / (len(df) * len(df.columns)),
                'collection_success': True
            }
            
            step_duration = time.time() - step_start
            
            result = {
                'component': 'data_collection',
                'status': 'SUCCESS',
                'duration_seconds': step_duration,
                'dataset_path': dataset_path,
                'records_collected': len(df),
                'nodes_monitored': df['node_id'].nunique(),
                'quality_metrics': quality_metrics,
                'config': collection_config
            }
            
            logger.info(f"  ‚úÖ Data Collection completed - {len(df)} records from {df['node_id'].nunique()} nodes ({step_duration:.1f}s)")
            return result
            
        except Exception as e:
            logger.error(f"  ‚ùå Data Collection failed: {e}")
            return {
                'component': 'data_collection',
                'status': 'FAILED',
                'error': str(e),
                'duration_seconds': time.time() - step_start
            }
    
    def simulate_preprocessing(self, raw_dataset_path: str) -> Dict[str, Any]:
        """
        Simulate Component 2: Preprocessing and Feature Engineering
        """
        logger.info("üîÑ STEP 2: Preprocessing Component - Starting...")
        step_start = time.time()
        
        try:
            # Load raw data
            df = pd.read_parquet(raw_dataset_path)
            logger.info(f"  üìä Loaded raw dataset: {df.shape}")
            
            # Data cleaning
            logger.info("  üßπ Performing data cleaning...")
            initial_size = len(df)
            df = df.drop_duplicates()
            df = df.dropna()
            
            # Feature Engineering
            logger.info("  ‚öôÔ∏è  Performing feature engineering...")
            time.sleep(1)  # Simulate processing
            
            # Time-based features
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['is_business_hours'] = ((df['hour'] >= 8) & (df['hour'] <= 18) & (df['day_of_week'] < 5)).astype(int)
            
            # Resource features
            df['resource_pressure'] = df['cpu_utilization'] + df['memory_utilization']
            df['load_efficiency'] = df['load_1min'] / (df['cpu_utilization'] + 0.01)
            
            # Rolling features per node
            df = df.sort_values(['node_id', 'timestamp'])
            for node in df['node_id'].unique():
                node_mask = df['node_id'] == node
                df.loc[node_mask, 'cpu_1h_avg'] = df.loc[node_mask, 'cpu_utilization'].rolling(12, min_periods=1).mean()
                df.loc[node_mask, 'memory_1h_avg'] = df.loc[node_mask, 'memory_utilization'].rolling(12, min_periods=1).mean()
            
            # Target variables
            df = df.sort_values(['node_id', 'timestamp'])
            for node in df['node_id'].unique():
                node_mask = df['node_id'] == node
                df.loc[node_mask, 'cpu_target_1h'] = df.loc[node_mask, 'cpu_utilization'].shift(-12)
                df.loc[node_mask, 'memory_target_1h'] = df.loc[node_mask, 'memory_utilization'].shift(-12)
            
            # Additional targets
            df['placement_optimal'] = ((df['cpu_utilization'] < 0.7) & 
                                      (df['memory_utilization'] < 0.8) & 
                                      (df['load_1min'] < 3.0)).astype(int)
            
            df['is_anomaly'] = ((df['cpu_utilization'] > df['cpu_utilization'].quantile(0.95)) | 
                               (df['memory_utilization'] > df['memory_utilization'].quantile(0.95))).astype(int)
            
            # Remove NaN from targets
            df = df.dropna()
            
            # Train-validation split
            validation_split = 0.2
            split_point = int(len(df) * (1 - validation_split))
            train_df = df.iloc[:split_point]
            val_df = df.iloc[split_point:]
            
            # Save processed datasets
            processed_path = f"{self.execution_dir}/processed_dataset.parquet"
            train_path = f"{self.execution_dir}/train_dataset.parquet"
            val_path = f"{self.execution_dir}/validation_dataset.parquet"
            
            df.to_parquet(processed_path, index=False)
            train_df.to_parquet(train_path, index=False)
            val_df.to_parquet(val_path, index=False)
            
            # Feature metadata
            feature_names = [
                'cpu_utilization', 'memory_utilization', 'load_1min', 'pod_count',
                'disk_usage_percent', 'hour', 'day_of_week', 'is_business_hours',
                'resource_pressure', 'load_efficiency', 'cpu_1h_avg', 'memory_1h_avg'
            ]
            
            step_duration = time.time() - step_start
            
            result = {
                'component': 'preprocessing',
                'status': 'SUCCESS',
                'duration_seconds': step_duration,
                'input_records': initial_size,
                'output_records': len(df),
                'train_records': len(train_df),
                'validation_records': len(val_df),
                'features_created': len(feature_names),
                'processed_dataset_path': processed_path,
                'train_dataset_path': train_path,
                'validation_dataset_path': val_path,
                'feature_names': feature_names
            }
            
            logger.info(f"  ‚úÖ Preprocessing completed - {len(feature_names)} features, {len(train_df)} train, {len(val_df)} val ({step_duration:.1f}s)")
            return result
            
        except Exception as e:
            logger.error(f"  ‚ùå Preprocessing failed: {e}")
            return {
                'component': 'preprocessing',
                'status': 'FAILED',
                'error': str(e),
                'duration_seconds': time.time() - step_start
            }
    
    def simulate_trio_training(self, train_path: str, val_path: str) -> Dict[str, Any]:
        """
        Simulate Component 3: Trio Training (Parallel)
        """
        logger.info("üîÑ STEP 3: Trio Training Component - Starting PARALLEL execution...")
        step_start = time.time()
        
        try:
            # Load datasets
            train_df = pd.read_parquet(train_path)
            val_df = pd.read_parquet(val_path)
            
            feature_names = [
                'cpu_utilization', 'memory_utilization', 'load_1min', 'pod_count',
                'disk_usage_percent', 'hour', 'day_of_week', 'is_business_hours',
                'resource_pressure', 'load_efficiency', 'cpu_1h_avg', 'memory_1h_avg'
            ]
            
            # Simulate parallel training of 3 algorithms
            trio_results = {}
            
            # 1. XGBoost "Le Proph√®te" Training
            logger.info("  üîÆ Training XGBoost Le Proph√®te...")
            time.sleep(3)  # Simulate training time
            
            # Simulate XGBoost performance
            xgb_cpu_r2 = np.random.uniform(0.82, 0.92)
            xgb_memory_r2 = np.random.uniform(0.85, 0.95)
            xgb_avg_r2 = (xgb_cpu_r2 + xgb_memory_r2) / 2
            xgb_business_score = min(100, xgb_avg_r2 * 100)
            
            trio_results['xgboost'] = {
                'algorithm': 'XGBoost',
                'expert_name': 'Le Proph√®te',
                'cpu_r2_score': xgb_cpu_r2,
                'memory_r2_score': xgb_memory_r2,
                'average_r2': xgb_avg_r2,
                'business_score': xgb_business_score,
                'model_path': f"{self.execution_dir}/xgboost_model.pkl",
                'training_samples': len(train_df),
                'validation_samples': len(val_df)
            }
            
            # 2. Q-Learning "L'Optimiseur" Training  
            logger.info("  üéØ Training Q-Learning L'Optimiseur...")
            time.sleep(2)  # Simulate training time
            
            # Simulate Q-Learning performance
            ql_accuracy = np.random.uniform(0.75, 0.85)
            ql_improvement = (ql_accuracy - 0.5) / 0.5 * 100
            ql_business_score = min(100, ql_improvement * 5)
            
            trio_results['qlearning'] = {
                'algorithm': 'Q-Learning',
                'expert_name': "L'Optimiseur",
                'accuracy': ql_accuracy,
                'improvement_vs_random': ql_improvement,
                'business_score': ql_business_score,
                'model_path': f"{self.execution_dir}/qlearning_model.pkl",
                'training_samples': len(train_df),
                'validation_samples': len(val_df)
            }
            
            # 3. Isolation Forest "Le D√©tective" Training
            logger.info("  üïµÔ∏è Training Isolation Forest Le D√©tective...")
            time.sleep(2)  # Simulate training time
            
            # Simulate Isolation Forest performance
            iso_precision = np.random.uniform(0.80, 0.95)
            iso_recall = np.random.uniform(0.75, 0.90)
            iso_f1 = 2 * (iso_precision * iso_recall) / (iso_precision + iso_recall)
            iso_business_score = min(100, iso_f1 * 100)
            
            trio_results['isolation'] = {
                'algorithm': 'Isolation Forest',
                'expert_name': 'Le D√©tective',
                'precision': iso_precision,
                'recall': iso_recall,
                'f1_score': iso_f1,
                'business_score': iso_business_score,
                'model_path': f"{self.execution_dir}/isolation_model.pkl",
                'training_samples': len(train_df),
                'validation_samples': len(val_df)
            }
            
            # Create model files (simulated)
            for algorithm, results in trio_results.items():
                model_data = {
                    'algorithm': algorithm,
                    'performance': results,
                    'feature_names': feature_names,
                    'training_date': datetime.now().isoformat()
                }
                with open(results['model_path'], 'w') as f:
                    json.dump(model_data, f, indent=2, default=str)
            
            step_duration = time.time() - step_start
            
            # Calculate average business score
            avg_business_score = np.mean([r['business_score'] for r in trio_results.values()])
            
            result = {
                'component': 'trio_training',
                'status': 'SUCCESS',
                'duration_seconds': step_duration,
                'algorithms_trained': list(trio_results.keys()),
                'trio_results': trio_results,
                'average_business_score': avg_business_score,
                'parallel_execution': True
            }
            
            logger.info(f"  ‚úÖ Trio Training completed - Avg business score: {avg_business_score:.1f}/100 ({step_duration:.1f}s)")
            for alg, res in trio_results.items():
                logger.info(f"    ‚Ä¢ {res['expert_name']}: {res['business_score']:.1f}/100")
            
            return result
            
        except Exception as e:
            logger.error(f"  ‚ùå Trio Training failed: {e}")
            return {
                'component': 'trio_training',
                'status': 'FAILED',
                'error': str(e),
                'duration_seconds': time.time() - step_start
            }
    
    def simulate_trio_validation(self, trio_results: Dict[str, Any], val_path: str) -> Dict[str, Any]:
        """
        Simulate Component 4: Trio Validation and Integration Testing
        """
        logger.info("üîÑ STEP 4: Trio Validation Component - Starting...")
        step_start = time.time()
        
        try:
            val_df = pd.read_parquet(val_path)
            logger.info(f"  üìä Validating trio integration on {len(val_df)} samples...")
            
            # Performance thresholds
            thresholds = {
                'xgboost_accuracy': 0.85,
                'qlearning_improvement': 0.15,
                'isolation_detection_rate': 0.85,
                'trio_integration_score': 75.0
            }
            
            # Individual model validation
            individual_scores = {}
            
            # XGBoost validation
            xgb_avg_r2 = trio_results['xgboost']['average_r2']
            individual_scores['xgboost'] = {
                'performance_metric': xgb_avg_r2,
                'business_score': trio_results['xgboost']['business_score'],
                'meets_threshold': xgb_avg_r2 >= thresholds['xgboost_accuracy']
            }
            
            # Q-Learning validation
            ql_improvement = trio_results['qlearning']['improvement_vs_random'] / 100
            individual_scores['qlearning'] = {
                'performance_metric': ql_improvement,
                'business_score': trio_results['qlearning']['business_score'],
                'meets_threshold': ql_improvement >= thresholds['qlearning_improvement']
            }
            
            # Isolation validation
            iso_f1 = trio_results['isolation']['f1_score']
            individual_scores['isolation'] = {
                'performance_metric': iso_f1,
                'business_score': trio_results['isolation']['business_score'],
                'meets_threshold': iso_f1 >= thresholds['isolation_detection_rate']
            }
            
            # Integration testing - Trio fusion simulation
            logger.info("  üîÑ Testing trio integration and fusion logic...")
            time.sleep(1)
            
            # Simulate fusion decisions
            fusion_test_samples = min(50, len(val_df))
            fusion_results = []
            
            for i in range(fusion_test_samples):
                # Simulate trio decision fusion
                prediction_score = np.random.uniform(0.3, 0.9)
                optimization_score = np.random.uniform(0.4, 0.8)
                anomaly_safe_score = np.random.uniform(0.5, 0.95)
                
                # Weighted fusion
                weights = {'prediction': 0.35, 'optimization': 0.35, 'anomaly': 0.30}
                fusion_score = (
                    prediction_score * weights['prediction'] +
                    optimization_score * weights['optimization'] + 
                    anomaly_safe_score * weights['anomaly']
                )
                
                decision = "ACCEPT" if fusion_score > 0.6 else "REJECT"
                fusion_results.append({
                    'fusion_score': fusion_score,
                    'decision': decision
                })
            
            # Analyze fusion performance
            accept_rate = sum(1 for r in fusion_results if r['decision'] == 'ACCEPT') / len(fusion_results)
            avg_fusion_score = np.mean([r['fusion_score'] for r in fusion_results])
            
            # Calculate trio integration score
            models_meeting_threshold = sum(1 for score in individual_scores.values() if score['meets_threshold'])
            avg_individual_score = np.mean([score['business_score'] for score in individual_scores.values()])
            
            # Integration bonus
            integration_bonus = 10 if models_meeting_threshold >= 2 else -10
            trio_integration_score = min(100, max(0, avg_individual_score + integration_bonus))
            
            # GO/NO-GO decision
            meets_threshold = trio_integration_score >= thresholds['trio_integration_score']
            go_no_go = "GO" if meets_threshold else "NO-GO"
            
            # Save validation results
            if go_no_go == "GO":
                # Prepare validated models
                validated_models_dir = f"{self.execution_dir}/validated_models"
                os.makedirs(validated_models_dir, exist_ok=True)
                
                deployment_manifest = {
                    'validation_passed': True,
                    'trio_integration_score': trio_integration_score,
                    'validation_timestamp': datetime.now().isoformat(),
                    'models': {
                        'xgboost': 'xgboost_model.pkl',
                        'qlearning': 'qlearning_model.pkl', 
                        'isolation': 'isolation_model.pkl'
                    },
                    'deployment_ready': True
                }
                
                with open(f"{validated_models_dir}/deployment_manifest.json", 'w') as f:
                    json.dump(deployment_manifest, f, indent=2)
            
            step_duration = time.time() - step_start
            
            result = {
                'component': 'trio_validation',
                'status': 'SUCCESS',
                'duration_seconds': step_duration,
                'individual_scores': individual_scores,
                'models_meeting_threshold': models_meeting_threshold,
                'trio_integration_score': trio_integration_score,
                'fusion_test_samples': len(fusion_results),
                'avg_fusion_score': avg_fusion_score,
                'accept_rate': accept_rate,
                'go_no_go_decision': go_no_go,
                'meets_performance_threshold': meets_threshold,
                'performance_thresholds': thresholds
            }
            
            logger.info(f"  ‚úÖ Trio Validation completed - Integration score: {trio_integration_score:.1f}/100")
            logger.info(f"    Models meeting threshold: {models_meeting_threshold}/3")
            logger.info(f"    Decision: {go_no_go}")
            
            return result
            
        except Exception as e:
            logger.error(f"  ‚ùå Trio Validation failed: {e}")
            return {
                'component': 'trio_validation',
                'status': 'FAILED',
                'error': str(e),
                'duration_seconds': time.time() - step_start
            }
    
    def simulate_kserve_deployment(self, go_no_go: str) -> Dict[str, Any]:
        """
        Simulate Component 5: KServe Deployment (Conditional)
        """
        logger.info("üîÑ STEP 5: KServe Deployment Component - Starting...")
        step_start = time.time()
        
        try:
            if go_no_go != "GO":
                logger.info("  ‚è≠Ô∏è  Skipping deployment - Validation did not pass")
                return {
                    'component': 'kserve_deployment',
                    'status': 'SKIPPED',
                    'reason': 'Validation failed - GO/NO-GO decision was NO-GO',
                    'duration_seconds': time.time() - step_start
                }
            
            logger.info("  üöÄ Deploying trio models to KServe...")
            
            # Simulate deployment of 3 InferenceServices
            trio_services = {
                'xgboost-predictor': {
                    'expert': 'Le Proph√®te',
                    'port': 8080,
                    'model_file': 'xgboost_model.pkl'
                },
                'qlearning-optimizer': {
                    'expert': "L'Optimiseur",
                    'port': 8081,
                    'model_file': 'qlearning_model.pkl'
                },
                'isolation-detector': {
                    'expert': 'Le D√©tective',
                    'port': 8082,
                    'model_file': 'isolation_model.pkl'
                }
            }
            
            deployment_results = {}
            service_endpoints = {}
            
            for service_name, config in trio_services.items():
                logger.info(f"    üì¶ Deploying {service_name} ({config['expert']})...")
                time.sleep(1)  # Simulate deployment time
                
                # Simulate deployment success
                deployment_success = np.random.choice([True, False], p=[0.85, 0.15])  # 85% success rate
                
                deployment_results[service_name] = {
                    'deployed': deployment_success,
                    'expert_name': config['expert'],
                    'port': config['port'],
                    'status': 'READY' if deployment_success else 'FAILED',
                    'deployment_time': datetime.now().isoformat()
                }
                
                if deployment_success:
                    service_endpoints[service_name] = {
                        'url': f'http://{service_name}.ml-scheduler.svc.cluster.local',
                        'port': config['port'],
                        'health_endpoint': f'/v1/models/{service_name}',
                        'predict_endpoint': f'/v1/models/{service_name}:predict'
                    }
            
            # Health checks simulation
            logger.info("  ü©∫ Performing health checks...")
            time.sleep(1)
            
            health_results = {}
            all_healthy = True
            
            for service_name, endpoint in service_endpoints.items():
                # Simulate health check
                healthy = np.random.choice([True, False], p=[0.9, 0.1])  # 90% health success
                
                health_results[service_name] = {
                    'healthy': healthy,
                    'response_time_ms': np.random.randint(20, 80),
                    'check_time': datetime.now().isoformat()
                }
                
                if not healthy:
                    all_healthy = False
            
            services_deployed = sum(1 for r in deployment_results.values() if r['deployed'])
            deployment_success = services_deployed == len(trio_services)
            
            step_duration = time.time() - step_start
            
            result = {
                'component': 'kserve_deployment',
                'status': 'SUCCESS' if (deployment_success and services_deployed >= 2) else 'PARTIAL',
                'duration_seconds': step_duration,
                'services_deployed': services_deployed,
                'total_services': len(trio_services),
                'deployment_success': deployment_success,
                'all_services_healthy': all_healthy,
                'deployment_details': deployment_results,
                'service_endpoints': service_endpoints,
                'health_results': health_results,
                'namespace': 'ml-scheduler'
            }
            
            logger.info(f"  ‚úÖ KServe Deployment completed - {services_deployed}/{len(trio_services)} services deployed")
            if all_healthy:
                logger.info("    ü©∫ All services healthy")
            else:
                logger.info("    ‚ö†Ô∏è  Some health check issues detected")
            
            return result
            
        except Exception as e:
            logger.error(f"  ‚ùå KServe Deployment failed: {e}")
            return {
                'component': 'kserve_deployment',
                'status': 'FAILED',
                'error': str(e),
                'duration_seconds': time.time() - step_start
            }
    
    def execute_complete_pipeline(self) -> Dict[str, Any]:
        """
        Execute complete pipeline end-to-end
        """
        print("="*80)
        print("üöÄ ML-SCHEDULER PIPELINE EXECUTION - √âTAPE 7")
        print("D√©monstration compl√®te pipeline Kubeflow orchestr√©")
        print("="*80)
        
        pipeline_start = time.time()
        
        try:
            # Step 1: Data Collection
            step1_result = self.simulate_data_collection()
            self.execution_results['components_results']['data_collection'] = step1_result
            
            if step1_result['status'] != 'SUCCESS':
                raise Exception(f"Data Collection failed: {step1_result.get('error', 'Unknown error')}")
            
            # Step 2: Preprocessing
            step2_result = self.simulate_preprocessing(step1_result['dataset_path'])
            self.execution_results['components_results']['preprocessing'] = step2_result
            
            if step2_result['status'] != 'SUCCESS':
                raise Exception(f"Preprocessing failed: {step2_result.get('error', 'Unknown error')}")
            
            # Step 3: Trio Training (Parallel)
            step3_result = self.simulate_trio_training(
                step2_result['train_dataset_path'],
                step2_result['validation_dataset_path']
            )
            self.execution_results['components_results']['trio_training'] = step3_result
            
            if step3_result['status'] != 'SUCCESS':
                raise Exception(f"Trio Training failed: {step3_result.get('error', 'Unknown error')}")
            
            # Step 4: Trio Validation
            step4_result = self.simulate_trio_validation(
                step3_result['trio_results'],
                step2_result['validation_dataset_path']
            )
            self.execution_results['components_results']['trio_validation'] = step4_result
            
            if step4_result['status'] != 'SUCCESS':
                raise Exception(f"Trio Validation failed: {step4_result.get('error', 'Unknown error')}")
            
            # Step 5: KServe Deployment (Conditional)
            step5_result = self.simulate_kserve_deployment(step4_result['go_no_go_decision'])
            self.execution_results['components_results']['kserve_deployment'] = step5_result
            
            # Pipeline completion
            pipeline_duration = time.time() - pipeline_start
            
            # Calculate overall metrics
            total_components = len(self.pipeline_config['components'])
            successful_components = sum(1 for r in self.execution_results['components_results'].values() 
                                      if r['status'] in ['SUCCESS', 'SKIPPED'])
            
            pipeline_success = successful_components == total_components
            
            self.execution_results.update({
                'status': 'SUCCESS' if pipeline_success else 'FAILED',
                'total_duration_seconds': pipeline_duration,
                'successful_components': successful_components,
                'total_components': total_components,
                'success_rate': successful_components / total_components,
                'end_time': datetime.now().isoformat()
            })
            
            # Performance metrics
            self.execution_results['performance_metrics'] = {
                'pipeline_duration_minutes': pipeline_duration / 60,
                'average_component_duration': np.mean([
                    r['duration_seconds'] for r in self.execution_results['components_results'].values()
                    if 'duration_seconds' in r
                ]),
                'trio_business_score': step3_result.get('average_business_score', 0),
                'integration_score': step4_result.get('trio_integration_score', 0),
                'deployment_success': step5_result['status'] in ['SUCCESS', 'PARTIAL', 'SKIPPED']
            }
            
            # Save execution results
            results_path = f"{self.execution_dir}/pipeline_execution_results.json"
            with open(results_path, 'w') as f:
                json.dump(self.execution_results, f, indent=2, default=str)
            
            return self.execution_results
            
        except Exception as e:
            pipeline_duration = time.time() - pipeline_start
            
            self.execution_results.update({
                'status': 'FAILED',
                'error': str(e),
                'total_duration_seconds': pipeline_duration,
                'end_time': datetime.now().isoformat()
            })
            
            logger.error(f"Pipeline execution failed: {e}")
            return self.execution_results
    
    def display_execution_summary(self, results: Dict[str, Any]):
        """Display comprehensive execution summary"""
        
        print("\n" + "="*80)
        print("üìä PIPELINE EXECUTION RESULTS")
        print("="*80)
        
        print(f"Pipeline ID: {results['pipeline_id']}")
        print(f"Status: {'‚úÖ SUCCESS' if results['status'] == 'SUCCESS' else '‚ùå FAILED'}")
        print(f"Duration: {results['total_duration_seconds']:.1f} seconds ({results['total_duration_seconds']/60:.1f} minutes)")
        
        if 'success_rate' in results:
            print(f"Success Rate: {results['success_rate']:.1%} ({results['successful_components']}/{results['total_components']} components)")
        
        print("\nüß© COMPONENT RESULTS:")
        for comp_name, comp_result in results['components_results'].items():
            status_icon = "‚úÖ" if comp_result['status'] == 'SUCCESS' else "‚è≠Ô∏è " if comp_result['status'] == 'SKIPPED' else "‚ùå"
            duration = comp_result.get('duration_seconds', 0)
            print(f"  {status_icon} {comp_name.replace('_', ' ').title()}: {comp_result['status']} ({duration:.1f}s)")
            
            # Component-specific details
            if comp_name == 'data_collection' and comp_result['status'] == 'SUCCESS':
                print(f"    üìä {comp_result['records_collected']} records from {comp_result['nodes_monitored']} nodes")
            
            elif comp_name == 'preprocessing' and comp_result['status'] == 'SUCCESS':
                print(f"    ‚öôÔ∏è  {comp_result['features_created']} features, {comp_result['train_records']} train samples")
            
            elif comp_name == 'trio_training' and comp_result['status'] == 'SUCCESS':
                print(f"    üéØ Avg business score: {comp_result['average_business_score']:.1f}/100")
                for alg, alg_result in comp_result['trio_results'].items():
                    print(f"      ‚Ä¢ {alg_result['expert_name']}: {alg_result['business_score']:.1f}/100")
            
            elif comp_name == 'trio_validation' and comp_result['status'] == 'SUCCESS':
                print(f"    üìã Integration score: {comp_result['trio_integration_score']:.1f}/100 - {comp_result['go_no_go_decision']}")
                print(f"    ‚úì Models meeting threshold: {comp_result['models_meeting_threshold']}/3")
            
            elif comp_name == 'kserve_deployment' and comp_result['status'] == 'SUCCESS':
                print(f"    üöÄ Services deployed: {comp_result['services_deployed']}/{comp_result['total_services']}")
                if comp_result.get('all_services_healthy'):
                    print(f"    ü©∫ All services healthy")
        
        if 'performance_metrics' in results:
            print(f"\nüìà PERFORMANCE METRICS:")
            perf = results['performance_metrics']
            print(f"  ‚Ä¢ Pipeline Duration: {perf['pipeline_duration_minutes']:.1f} minutes")
            print(f"  ‚Ä¢ Trio Business Score: {perf['trio_business_score']:.1f}/100")
            print(f"  ‚Ä¢ Integration Score: {perf['integration_score']:.1f}/100")
            print(f"  ‚Ä¢ Deployment Success: {'‚úÖ' if perf['deployment_success'] else '‚ùå'}")
        
        print(f"\nüìÅ Execution artifacts saved in: {self.execution_dir}")
        
        print("\n" + "="*80)
        if results['status'] == 'SUCCESS':
            print("üéâ PIPELINE EXECUTION SUCCESSFUL!")
            print("ML-Scheduler trio pipeline orchestration demonstrated successfully")
        else:
            print("‚ö†Ô∏è  PIPELINE EXECUTION ENCOUNTERED ISSUES")
            if 'error' in results:
                print(f"Error: {results['error']}")
        print("="*80)


def main():
    """
    Main execution function
    """
    # Create pipeline executor
    executor = PipelineExecutor()
    
    # Execute complete pipeline
    results = executor.execute_complete_pipeline()
    
    # Display summary
    executor.display_execution_summary(results)
    
    # Return success/failure
    return results['status'] == 'SUCCESS'


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)